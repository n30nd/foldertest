[2024-11-06 00:37:35,001][flwr][INFO] - Starting Flower simulation, config: num_rounds=15, no round_timeout
[2024-11-06 00:37:38,816][flwr][INFO] - Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:192.168.1.223': 1.0, 'CPU': 12.0, 'memory': 5804384256.0, 'object_store_memory': 2902192128.0}
[2024-11-06 00:37:38,816][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-11-06 00:37:38,816][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-11-06 00:37:38,824][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 6 actors
[2024-11-06 00:37:38,825][flwr][INFO] - [INIT]
[2024-11-06 00:37:38,825][flwr][INFO] - Requesting initial parameters from one random client
[2024-11-06 00:37:41,531][flwr][INFO] - Received initial parameters from one random client
[2024-11-06 00:37:41,531][flwr][INFO] - Starting evaluation of initial global parameters
[2024-11-06 00:37:46,084][flwr][INFO] - initial parameters (loss, other metrics): 0.02233456829801584, {'accuracy': 0.375}
[2024-11-06 00:37:46,084][flwr][INFO] - 
[2024-11-06 00:37:46,084][flwr][INFO] - [ROUND 1]
[2024-11-06 00:37:46,085][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:38:03,814][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-11-06 00:38:03,885][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2024-11-06 00:38:09,003][flwr][INFO] - fit progress: (1, 0.020011576274648692, {'accuracy': 0.625}, 22.918619812000543)
[2024-11-06 00:38:09,003][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:38:15,343][flwr][INFO] - aggregate_evaluate: received 10 results and 0 failures
[2024-11-06 00:38:15,343][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.7307692307692308
[2024-11-06 00:38:15,343][flwr][INFO] - 
[2024-11-06 00:38:15,343][flwr][INFO] - [ROUND 2]
[2024-11-06 00:38:15,343][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:38:22,586][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:22,591][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:25,188][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:25,188][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:25,189][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:25,190][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:25,204][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:25,205][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:28,206][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:38:33,318][flwr][INFO] - fit progress: (2, 0.018360148016840983, {'accuracy': 0.6602564102564102}, 47.23355765800079)
[2024-11-06 00:38:33,318][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:38:37,474][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:37,490][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:38,487][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:38,487][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:39,468][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:38:39,468][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.8418803418803419
[2024-11-06 00:38:39,469][flwr][INFO] - 
[2024-11-06 00:38:39,469][flwr][INFO] - [ROUND 3]
[2024-11-06 00:38:39,469][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:38:44,893][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:44,894][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:45,936][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:45,943][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:48,130][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:48,131][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:38:48,131][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:48,132][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:38:51,093][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:38:56,378][flwr][INFO] - fit progress: (3, 0.012644151279416222, {'accuracy': 0.7916666666666666}, 70.29361172599965)
[2024-11-06 00:38:56,378][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:01,490][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:01,490][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:01,491][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:01,491][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:01,996][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:39:01,996][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9134615384615385
[2024-11-06 00:39:01,996][flwr][INFO] - 
[2024-11-06 00:39:01,996][flwr][INFO] - [ROUND 4]
[2024-11-06 00:39:01,996][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:06,958][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:06,992][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:07,969][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:07,975][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:09,680][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:09,681][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:09,686][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:09,686][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:13,311][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:39:18,047][flwr][INFO] - fit progress: (4, 0.012712451283079691, {'accuracy': 0.7948717948717948}, 91.9623765130018)
[2024-11-06 00:39:18,047][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:23,145][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:23,145][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:24,178][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2024-11-06 00:39:24,178][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9233934789490346
[2024-11-06 00:39:24,178][flwr][INFO] - 
[2024-11-06 00:39:24,179][flwr][INFO] - [ROUND 5]
[2024-11-06 00:39:24,179][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:30,620][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:30,621][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:32,134][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:32,134][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:32,135][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:32,138][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:32,138][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:32,138][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:35,893][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:39:41,553][flwr][INFO] - fit progress: (5, 0.014459151864195099, {'accuracy': 0.7852564102564102}, 115.46825806900233)
[2024-11-06 00:39:41,553][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:47,810][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:47,810][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:48,938][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2024-11-06 00:39:48,938][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9362139917695474
[2024-11-06 00:39:48,938][flwr][INFO] - 
[2024-11-06 00:39:48,938][flwr][INFO] - [ROUND 6]
[2024-11-06 00:39:48,939][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:39:54,265][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:54,271][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:55,310][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:55,322][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:56,242][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:56,253][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:39:57,318][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:39:57,319][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:00,916][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:40:05,778][flwr][INFO] - fit progress: (6, 0.017088724327130385, {'accuracy': 0.7708333333333334}, 139.6931687260003)
[2024-11-06 00:40:05,778][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:40:10,397][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:10,413][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:10,820][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:10,821][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:11,859][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:40:11,859][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9715099715099715
[2024-11-06 00:40:11,859][flwr][INFO] - 
[2024-11-06 00:40:11,859][flwr][INFO] - [ROUND 7]
[2024-11-06 00:40:11,859][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:40:16,494][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:16,501][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:17,743][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:17,743][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:19,566][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:19,567][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:19,574][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:19,578][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:22,747][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:40:27,561][flwr][INFO] - fit progress: (7, 0.01541686452811775, {'accuracy': 0.7932692307692307}, 161.47687330700137)
[2024-11-06 00:40:27,562][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:40:32,692][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:32,693][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:32,694][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:32,695][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:33,639][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:40:33,639][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9378561253561253
[2024-11-06 00:40:33,639][flwr][INFO] - 
[2024-11-06 00:40:33,639][flwr][INFO] - [ROUND 8]
[2024-11-06 00:40:33,639][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:40:38,220][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:38,221][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:39,337][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:39,348][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:40,981][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:40,990][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:40,990][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:40,992][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:44,084][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:40:48,669][flwr][INFO] - fit progress: (8, 0.02232774612060987, {'accuracy': 0.7596153846153846}, 182.58463527099957)
[2024-11-06 00:40:48,669][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:40:53,776][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:40:53,776][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:40:54,806][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2024-11-06 00:40:54,806][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9830642608420388
[2024-11-06 00:40:54,806][flwr][INFO] - 
[2024-11-06 00:40:54,806][flwr][INFO] - [ROUND 9]
[2024-11-06 00:40:54,806][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:00,050][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:00,070][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:01,149][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:01,188][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:02,573][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:02,574][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:02,578][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:02,578][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:05,499][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:41:10,372][flwr][INFO] - fit progress: (9, 0.021068259040102698, {'accuracy': 0.7676282051282052}, 204.28748360300233)
[2024-11-06 00:41:10,372][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:15,388][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:15,388][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:15,389][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:15,390][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:16,361][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:41:16,361][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.951923076923077
[2024-11-06 00:41:16,361][flwr][INFO] - 
[2024-11-06 00:41:16,362][flwr][INFO] - [ROUND 10]
[2024-11-06 00:41:16,362][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:21,284][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:21,290][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:22,358][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:22,358][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:24,166][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:24,167][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:28,107][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-11-06 00:41:33,101][flwr][INFO] - fit progress: (10, 0.020247842464297533, {'accuracy': 0.7884615384615384}, 227.01678988900312)
[2024-11-06 00:41:33,101][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:38,253][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:38,253][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:38,254][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:38,254][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:39,243][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:41:39,243][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9615384615384616
[2024-11-06 00:41:39,243][flwr][INFO] - 
[2024-11-06 00:41:39,243][flwr][INFO] - [ROUND 11]
[2024-11-06 00:41:39,243][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:44,223][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:44,223][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:46,148][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:46,166][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:47,098][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:47,098][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:47,103][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:47,104][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:41:49,847][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:41:54,735][flwr][INFO] - fit progress: (11, 0.021593118461075787, {'accuracy': 0.7772435897435898}, 248.65052931600076)
[2024-11-06 00:41:54,735][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:41:59,787][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:41:59,788][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:00,801][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2024-11-06 00:42:00,801][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9273504273504274
[2024-11-06 00:42:00,801][flwr][INFO] - 
[2024-11-06 00:42:00,801][flwr][INFO] - [ROUND 12]
[2024-11-06 00:42:00,802][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:42:05,849][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:05,849][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:07,073][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:07,074][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:08,546][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:08,546][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:08,554][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:08,554][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:11,448][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:42:16,229][flwr][INFO] - fit progress: (12, 0.02057864583986931, {'accuracy': 0.7836538461538461}, 270.14490586800093)
[2024-11-06 00:42:16,230][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:42:20,386][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:20,393][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:21,298][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:21,298][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:22,343][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:42:22,343][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9615384615384617
[2024-11-06 00:42:22,343][flwr][INFO] - 
[2024-11-06 00:42:22,343][flwr][INFO] - [ROUND 13]
[2024-11-06 00:42:22,344][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:42:27,174][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:27,195][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:29,152][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:29,152][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:30,014][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:30,014][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:30,014][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:30,015][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:30,032][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:30,033][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:31,439][flwr][INFO] - aggregate_fit: received 5 results and 5 failures
[2024-11-06 00:42:36,297][flwr][INFO] - fit progress: (13, 0.02283181227558555, {'accuracy': 0.7676282051282052}, 290.2128157750012)
[2024-11-06 00:42:36,297][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:42:41,503][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:41,504][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:41,506][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:41,506][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:42,465][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:42:42,465][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9711538461538461
[2024-11-06 00:42:42,465][flwr][INFO] - 
[2024-11-06 00:42:42,465][flwr][INFO] - [ROUND 14]
[2024-11-06 00:42:42,465][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:42:47,954][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:47,955][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:49,251][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:49,267][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:50,874][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:50,874][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:50,880][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:42:50,881][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:42:54,370][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:42:59,216][flwr][INFO] - fit progress: (14, 0.019213622288915735, {'accuracy': 0.7852564102564102}, 313.131373127002)
[2024-11-06 00:42:59,216][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:43:04,583][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:04,584][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:04,585][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:04,585][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:05,174][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2024-11-06 00:43:05,175][root][INFO] - EVALUATE_METRICS_AGGREGATION: 0.9567307692307693
[2024-11-06 00:43:05,175][flwr][INFO] - 
[2024-11-06 00:43:05,175][flwr][INFO] - [ROUND 15]
[2024-11-06 00:43:05,175][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-11-06 00:43:10,286][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:10,297][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:11,410][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:11,427][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:12,885][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:12,888][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:12,888][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:12,894][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-11-06 00:43:15,589][flwr][INFO] - aggregate_fit: received 6 results and 4 failures
[2024-11-06 00:43:20,167][flwr][INFO] - fit progress: (15, 0.026217686515361167, {'accuracy': 0.7692307692307693}, 334.0824509910017)
[2024-11-06 00:43:20,167][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 20)
[2024-11-06 00:43:25,076][flwr][ERROR] - Traceback (most recent call last):
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-11-06 00:43:25,076][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.1.223, ID: 48cfef326bc5a0b72bb6c034251210aa4399f562314657aa23ed8667) where the task (actor ID: 94d87badd4efe836e053fb7c01000000, name=ClientAppActor.__init__, pid=389746, memory used=0.10GB) was running was 14.84GB / 15.48GB (0.959084), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.223`. To see the logs of the worker, use `ray logs worker-51ad11b2039306983326009846ca4e55fe23450ef14559fd500ab441*out -ip 192.168.1.223. Top 10 memory users:
PID	MEM(GB)	COMMAND
388967	1.68	/media/namvq/Data/anaconda3/envs/env1/bin/python /media/namvq/Data/code_flwr/turorial1/main.py
376083	1.37	/usr/bin/python3 /usr/share/apport/apport-gtk
389743	0.68	ray::ClientAppActor.run
42279	0.62	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
41910	0.35	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
3957	0.28	/usr/share/code/code --type=renderer --crashpad-handler-pid=3922 --enable-crash-reporter=c382d37a-bb...
42008	0.27	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
151509	0.24	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=3114 --enable-...
4460	0.22	/usr/share/code/code /home/namvq/.vscode/extensions/ms-python.vscode-pylance-2024.11.1/dist/server.b...
389744	0.14	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
